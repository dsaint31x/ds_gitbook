# Vector Sapces

# 4.4 The Dimension of a Vector Space

## Theorem 9: 

If a vector space $$V$$ has a basis $$B=\left\{ \textbf{b}_1, \cdots, \textbf{b}_n \right\}$$,
then any set in $$V$$ containing more
than $$n$$ vectors must be linearly dependent.

### Proof: 

* Let $$\left\{ \textbf{u}_1, \cdots, \textbf{u}_p \right\}$$
  be a set in $$V$$ with more than
  $$n$$ vectors.

* The coordinate vectors $$[\textbf{u}_1]_B, \cdots,[\textbf{u}_p]_B$$
  form a linearly dependent set in $$\mathbb{R}^n$$,
  because there are
  more vectors ($$p$$) than entries ($$n$$) in each vector.

* So there exist scalars $$c_1, \cdots, c_p$$, not all zero, such that

$$
c_1[\textbf{u}_1]_B + \cdots + c_p [\textbf{u}_p]_B = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}
\text{The zero vector in }\mathbb{R}^n
$$

* Since the coordinate mapping is a linear transformation,

$$
\left[ c_1 \textbf{u}_1 + \cdots + c_p \textbf{u}_p \right]_B = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}
$$

* The zero vector on the right displays the $$n$$ weights
  needed to build the vector $$c_1 \textbf{u}_1 + \cdots + c_p \textbf{u}_p$$ 
  from the basis vectors in $$B$$.
* That is, $$c_1 \textbf{u}_1 + \cdots + c_p \textbf{u}_p = 0\textbf{b}_1 +\cdots 0\textbf{b}_n = \textbf{0}$$.
* Since the $$c_i$$ are not all zero, $$\left\{ \textbf{u}_1, \cdots, \textbf{u}_p \right\}$$
  is linearly dependent.

> Theorem 9 implies that if a vector space $$V$$ has a basis $$B=\left\{ \textbf{b}_1, \cdots, \textbf{b}_n \right\}$$
> then each linearly independent set in $$V$$ has no more than $$n$$ vectors.

## Theorem 10: 

If a vector space $$V$$ has a basis of $$n$$ vectors, 
then every basis of $$V$$ must consist of exactly $$n$$ vectors.

### Proof: 

* Let $$B_1$$ be a basis of $$n$$ vectors and $$B_2$$ be any other basis (of $$V$$).
* Since $$B_1$$ is a basis and $$B_2$$ is linearly independent,
  $$B_2$$ has no more than $$n$$ vectors, by Theorem 9.
* Also, since $$B_2$$ is a basis and $$B_1$$ is linearly independent,
  $$B_2$$ has at least $$n$$ vectors.
* Thus $$B_2$$ consists of exactly $$n$$ vectors.

## Definition

* If $$V$$ is spanned by a **finite set**, 
  then $$V$$ is said to be **finite-dimensional**, and 
* the **dimension** of $$V$$, written as **dim** $$V$$, 
  is the **number of vectors in a basis for $$V$$.** 
* The **dimension of the zero vector space** $$\left\{\textbf{0}\right\}$$ is defined to be zero. 
* If $$V$$ is not spanned by a finite set, then $$V$$ is said to be **infinite-dimensional**.

## Example 3

Find the dimension of the subspace

$$
H = 
\left\{ 
\left[
a-3b+6c \\
5a+4d \\
b-2c-d \\
5d
\right]:a,b,c,d \in \mathbb{R}
\right\}
$$

* $$H$$ is the set of all linear combinations of the vectors
$$
\textbf{v}_1 = \begin{bmatrix} 1 \\ 5 \\ 0 \\ 0 \end{bmatrix},
\textbf{v}_2 = \begin{bmatrix} -3 \\ 0 \\ 1 \\ 0 \end{bmatrix},
\textbf{v}_3 = \begin{bmatrix} 6 \\ 0 \\ -2 \\ 0 \end{bmatrix},
\textbf{v}_4 = \begin{bmatrix} 0 \\ 4 \\ -1 \\ 5 \end{bmatrix}
$$

* Clearly $$\textbf{v}_1 \ne \textbf{0}, \textbf{v}_2$$ is not a multiple of $$\textbf{v}_1$$,
  but $$\textbf{v}_3$$ is a multiple of $$\textbf{v}_2$$.
* By **the Spanning Set Theorem**, we may discard $$\textbf{v}_3$$ 
  and still have a set that spans $$H$$.
* Finally $$\textbf{v}_4$$ is not a linear combination of $$\textbf{v}_1$$ and $$\textbf{v}_2$$.
  So $$\left\{ \textbf{v}_1, \textbf{v}_2, \textbf{v}_4 \right\}$$ is linearly independent 
  and hence is a basis for H.
* Thus **dim **$$H$$=3.

## Theorem 11: 

Let $$H$$ be a subspace of a finite-dimensional vector space $$V$$. 
Any linearly independent set in $$H$$ can be expanded, if necessary, to a basis for $$H$$. 
Also, $$H$$ is finite-dimensional and $$\text{dim }H \le \text{dim }V$$.

### Proof:

* If $$H=\left\{ \textbf{0} \right\}$$,
  then certainly $$\text{dim }H=0 \le \text{dim }V$$.
* Otherwise, let $$S=\left\{ \textbf{u}_1, \cdots, \textbf{u}_k \right\}$$
  be any linearly independent set in $$H$$.
* If $$S$$ spans $$H$$, then $$S$$ is a basis for $$H$$.
* Otherwise, there is some $$\textbf{u}_{k+1} $$ in $$H$$ that is not in Span $$S$$.
* But then $$\left\{ \textbf{u}_1, \cdots \textbf{u}_k, \textbf{u}_{k+1} \right\}$$
  will be linearly independent,
  because no vector in the set can be a linear
  combination of vectors that precede it (by Theorem 4).
* So long as the new set does not span $$H$$, we can
  continue this process of expanding $$S$$ to a larger
  linearly independent set in $$H$$.
* But the number of vectors in a linearly independent expansion of $$S$$ 
  can never exceed the dimension of $$V$$, by Theorem 9.
* So eventually the expansion of $$S$$ will span $$H$$ and
  hence will be a basis for $$H$$, and $$\text{dim }H \le \text{dim }V$$.

## Theorem 12: Basis Theorem

Let $$V$$ be a $$p$$-dimensional vector space, $$p\ge 1$$.
Any linearly independent set of exactly $$p$$ elements in $$V$$ is automatically a basis for $$V$$. 
Any set of exactly $$p$$ elements that spans $$V$$ is automatically a basis for $$V$$.

### Proof: 

* By Theorem 11, a linearly independent set $$S$$ of $$p$$ elements can be extended to a basis for $$V$$.
* But that basis must contain exactly $$p$$ elements, since $$\text{dim }V=p$$.
* So $$S$$ must already be a basis for $$V$$.
* Now suppose that $$S$$ has $$p$$ elements and spans $$V$$.
* Since $$V$$ is nonzero, the **Spanning Set Theorem**
  implies that a subset $$S'$$ of $$S$$ is a basis of $$V$$.
* Since $$\text{dim }V=p, S'$$ must contain $$p$$ vectors.
* Hence $$S=S'$$.

## The Dimensions of Nul A and Col A

* Let $$A$$ be an $$m \times n$$ matrix, and suppose the equation $$A\textbf{x}=\textbf{0}$$ has $$k$$ free variables.
* A spanning set for **Nul **$$A$$ will produce 
  exactly $$k$$ linearly independent vectors—say, $$\textbf{u}_1, \cdots, \textbf{u}_k$$
  —one for each free variable.
* So $$\left\{ \textbf{u}_1, \cdots, \textbf{u}_k \right\}$$ is a basis for **Nul** $$A$$, 
  and **the number of free variables determines the size of the basis**.
* Thus, the dimension of **Nul** $$A$$ is the number of free variables in the equation $$A\textbf{x}=\textbf{0}$$ and the dimension of **Col** $$A$$ is **the number of pivot columns in** $$A$$.

## Example 5:

Find the dimensions of the null space and the column space of

$$
A=\begin{bmatrix}
-3 & 6 & -1 & 1 & -7 \\
1 & -2 & 2 & 3 & -1 \\
2 & -4 & 5 & 8 & -4 
\end{bmatrix}
$$

### Solution:

* Row reduce the augmented matrix $$\begin{bmatrix} A & \textbf{0} \end{bmatrix}$$
  to echelon form:

$$
\begin{bmatrix}
1 & -2 & 2 & 3 & -1 & 0 \\
0 &  0 & 1 & 2 & -2 & 0 \\
0 &  0 & 0 & 0 &  0 & 0 
\end{bmatrix}
$$

* There are three free variable— $$x_2, x_4$$, and $$x_5$$.
* Hence the dimension of **Nul **$$A$$ is 3.
* Also dim **Col** $$A$$ =2
  because $$A$$ has two pivot
  columns.
